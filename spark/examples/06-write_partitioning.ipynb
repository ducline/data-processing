{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducline/edit-data_processing/blob/main/spark/examples/06-write_partitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Write\n",
        "- .write\n",
        "- .format (parquet, csv, json)\n",
        "- options\n",
        "- spark.sql.sources.partitionOverwriteMode dynamic\n",
        "\n",
        "# Write Mode\n",
        "- overwrite - The overwrite mode is used to overwrite the existing file, alternatively, you can use SaveMode.Overwrite\n",
        "- append - To add the data to the existing file, alternatively, you can use SaveMode.Append\n",
        "- ignore - Ignores write operation when the file already exists, alternatively, you can use SaveMode.Ignore.\n",
        "- errorifexists or error - This is a default option when the file already exists, it returns an error, alternatively, you can use SaveMode.ErrorIfExists.\n",
        "\n",
        "# Partitioning\n",
        "Process to organize the data into multiple chunks based on some criteria.\n",
        "Partitions are organized in sub-folders.\n",
        "Partitioning improves performance in Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "e0aa4a23-c958-4777-feb8-81ebf020d1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3Cg2riVX3m"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83BBHcNJDmw4",
        "outputId": "0c3c11ea-8b6c-49e9-af5b-4ee034e1480f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z-caHS2MVX3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf30ce8-d12a-41fc-c68f-95b5b27da43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------+--------------------------+----------+-------------------------+------------------+---------------------+\n",
            "|address                                                |date                      |dob       |email                    |name              |phone                |\n",
            "+-------------------------------------------------------+--------------------------+----------+-------------------------+------------------+---------------------+\n",
            "|4973 Amber Ranch Apt. 834\\nStephensonmouth, CO 07625   |2024-05-03 11:37:52.167497|1918-06-08|odelacruz@example.org    |Dawn Ortega       |253.508.8977         |\n",
            "|883 King Villages\\nHamptonburgh, NJ 38378              |2024-05-03 09:31:21.718106|1993-02-26|yvega@example.com        |Arthur Mack       |2395131993           |\n",
            "|700 Nicole Path\\nSheilamouth, LA 62980                 |2024-05-03 20:57:51.270688|2014-11-29|torresrebecca@example.net|Justin Gonzalez   |9529406655           |\n",
            "|02437 Luna Highway Apt. 257\\nCervantesport, KS 08567   |2024-05-03 15:19:05.740055|2016-04-11|shelly25@example.org     |Catherine Scott   |(592)680-4984        |\n",
            "|41718 Donald Island Apt. 212\\nHernandezberg, AK 52302  |2024-05-02 08:21:44.077018|1929-08-07|candice41@example.net    |Jeremy Holmes     |001-950-609-8212x212 |\n",
            "|5696 Young Station Suite 257\\nPort Philipfurt, MH 76613|2024-05-01 15:15:03.788843|1923-09-23|sean61@example.net       |Matthew Richardson|821-984-5954         |\n",
            "|0231 Ward Prairie\\nWest Toddmouth, GU 81005            |2024-05-04 03:44:59.935719|1972-03-31|wyattalyssa@example.com  |Jessica Johnson   |(517)686-4146x548    |\n",
            "|727 Julia Parks\\nNorth Louisberg, PW 39632             |2024-05-04 11:12:08.318239|1958-08-21|ibarajas@example.com     |Scott Allen       |899-731-1448         |\n",
            "|343 Vazquez Heights\\nNorth Howardmouth, DE 12888       |2024-05-04 19:40:35.340748|1933-06-18|cody34@example.net       |Wayne Goodman     |485-385-3350         |\n",
            "|230 Laura Ridge Suite 258\\nSouth Kristin, ND 17931     |2024-05-01 16:56:37.889107|1983-07-11|fgreen@example.com       |Jamie Robinson    |+1-297-578-4389x09121|\n",
            "+-------------------------------------------------------+--------------------------+----------+-------------------------+------------------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "users = []\n",
        "for _ in range(50):\n",
        "    user = {\n",
        "        'date': fake.date_time_between_dates(datetime(2024, 5, 1), datetime(2024, 5, 5)),\n",
        "        'name': fake.name(),\n",
        "        'address': fake.address(),\n",
        "        'email': fake.email(),\n",
        "        'dob': fake.date_of_birth(),\n",
        "        'phone': fake.phone_number()\n",
        "    }\n",
        "    users.append(user)\n",
        "\n",
        "df = spark.createDataFrame(users)\n",
        "\n",
        "df.show(10, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGXjf6xpBj36"
      },
      "source": [
        "# Writing as PARQUET\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14stpbb4Bj37"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw5IIgebBj37",
        "outputId": "77b3d971-b3ab-4484-cbfe-378751bd04f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-1fc4cfc2-4469-4c7b-b71c-207cdf78ddbc-c000.snappy.parquet  _SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Writing as PARQUET with no partitions\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_no_partitions\"\n",
        "\n",
        "df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
        "\n",
        "!ls /content/write_partitioning/parquet_no_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with partitions\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_with_partitions\"\n",
        "\n",
        "# Creating partition column\n",
        "df = df.withColumn(\"date_part\", date_format(col(\"date\"), \"yyyyMMdd\"))\n",
        "\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\") # enable dynamic partition overwrite - only overwrites partitions that are coming in the dataframe\n",
        "\n",
        "(df#.where(\"date_part = '20240503'\")\n",
        " .write\n",
        " .mode(\"overwrite\")                                               # overwrites the entire path with the new data\n",
        " .partitionBy(\"date_part\")                                        # partition the data by column - creates sub-folders for each partition\n",
        " .format(\"parquet\")                                               # format of output\n",
        " .save(path))                                                     # path\n",
        "\n",
        "!ls /content/write_partitioning/parquet_with_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWX9WZbPHrL1",
        "outputId": "37cacbe2-22ef-4931-cc81-6b5530ff57d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'date_part=20240501'  'date_part=20240502'  'date_part=20240503'  'date_part=20240504'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking single partition\n",
        "spark.read.parquet(\"/content/write_partitioning/parquet_with_partitions/date_part=20240502\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B62qu87JsAB",
        "outputId": "8c8c88fb-bc62-49fe-c1b4-af7ed63ed619"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "|             address|                date|       dob|               email|              name|               phone|\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "|41718 Donald Isla...|2024-05-02 08:21:...|1929-08-07|candice41@example...|     Jeremy Holmes|001-950-609-8212x212|\n",
            "|66919 Collins Sky...|2024-05-02 07:43:...|1928-11-14|vburnett@example.org| William Blackburn|    946.743.0437x584|\n",
            "|222 Hannah Park A...|2024-05-02 09:19:...|1929-11-14|bennettanthony@ex...|      Heather Gray|+1-554-698-6296x1976|\n",
            "|57547 Kimberly Da...|2024-05-02 22:44:...|2012-02-10|  okirby@example.net|    Olivia Johnson|001-570-547-8859x679|\n",
            "|936 Christina Clu...|2024-05-02 01:57:...|1980-10-19|timothy34@example...|    Brittany White|        751.283.8413|\n",
            "|0538 Thomas Locks...|2024-05-02 07:46:...|1990-11-30|  xgrant@example.org|       Peter Lynch|        985-574-1855|\n",
            "|70219 Roberta Mew...|2024-05-02 17:52:...|2015-09-23|mcculloughjesse@e...|   Elizabeth Poole|    470-807-9208x564|\n",
            "|6902 Eric Place S...|2024-05-02 07:28:...|2012-01-31|kristina60@exampl...|Elizabeth Anderson|   235-568-0368x3032|\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing as CSV\n",
        "\n",
        "https://spark.apache.org/docs/3.5.1/sql-data-sources-csv.html"
      ],
      "metadata": {
        "id": "n8mTC5yeNV6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnAWUTeZO43Z",
        "outputId": "c4bbd7be-8abf-4638-9263-6331d26eb90f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/write_partitioning/csv_no_partitioning/\"\n",
        "\n",
        "# write as csv\n",
        "(df\n",
        "  .write\n",
        "  .format(\"csv\")\n",
        "  .mode(\"overwrite\")\n",
        "  .option(\"delimiter\", \"|\")\n",
        "  .option(\"header\", True)\n",
        "  .save(path))\n",
        "\n",
        "# listing files in the folder\n",
        "!ls /content/write_partitioning/csv_no_partitioning/\n",
        "\n",
        "# read as csv\n",
        "(spark\n",
        "  .read\n",
        "  .options(sep=\"|\", multiLine=True, header=True)\n",
        "  .csv(path)\n",
        "  .count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE6zC-HnNYAz",
        "outputId": "5ccbe3e8-3149-4be2-c35e-7873b79de322"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-0064c4ed-5cc5-4f4a-9d62-97d34bfc4e68-c000.csv  _SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing as JSON\n",
        "\n",
        "https://spark.apache.org/docs/3.5.1/sql-data-sources-json.html"
      ],
      "metadata": {
        "id": "ZAuM5-WcTtyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/write_partitioning/json_no_partitioning/\"\n",
        "\n",
        "# write as json\n",
        "(df\n",
        ".write\n",
        ".mode(\"overwrite\")\n",
        ".format(\"json\")\n",
        ".save(path))\n",
        "\n",
        "# listing files in the folder\n",
        "!ls /content/write_partitioning/json_no_partitioning/\n",
        "\n",
        "# read as json\n",
        "(spark\n",
        "  .read\n",
        "  .json(path)\n",
        "  .count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnNgwbtxTsW_",
        "outputId": "5c56560a-0de4-44e5-a6f2-2dc0e2a5c24a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-056371c8-33df-400b-835c-28462a32bdad-c000.json  _SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading json as text\n",
        "spark.read.text(path).show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3hYNCubT0ry",
        "outputId": "010956ab-9070-49b0-ffbd-6efa436e9e30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                                                                                |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{\"address\":\"4973 Amber Ranch Apt. 834\\nStephensonmouth, CO 07625\",\"date\":\"2024-05-03T11:37:52.167Z\",\"dob\":\"1918-06-08\",\"email\":\"odelacruz@example.org\",\"name\":\"Dawn Ortega\",\"phone\":\"253.508.8977\",\"date_part\":\"20240503\"}           |\n",
            "|{\"address\":\"883 King Villages\\nHamptonburgh, NJ 38378\",\"date\":\"2024-05-03T09:31:21.718Z\",\"dob\":\"1993-02-26\",\"email\":\"yvega@example.com\",\"name\":\"Arthur Mack\",\"phone\":\"2395131993\",\"date_part\":\"20240503\"}                            |\n",
            "|{\"address\":\"700 Nicole Path\\nSheilamouth, LA 62980\",\"date\":\"2024-05-03T20:57:51.270Z\",\"dob\":\"2014-11-29\",\"email\":\"torresrebecca@example.net\",\"name\":\"Justin Gonzalez\",\"phone\":\"9529406655\",\"date_part\":\"20240503\"}                   |\n",
            "|{\"address\":\"02437 Luna Highway Apt. 257\\nCervantesport, KS 08567\",\"date\":\"2024-05-03T15:19:05.740Z\",\"dob\":\"2016-04-11\",\"email\":\"shelly25@example.org\",\"name\":\"Catherine Scott\",\"phone\":\"(592)680-4984\",\"date_part\":\"20240503\"}       |\n",
            "|{\"address\":\"41718 Donald Island Apt. 212\\nHernandezberg, AK 52302\",\"date\":\"2024-05-02T08:21:44.077Z\",\"dob\":\"1929-08-07\",\"email\":\"candice41@example.net\",\"name\":\"Jeremy Holmes\",\"phone\":\"001-950-609-8212x212\",\"date_part\":\"20240502\"}|\n",
            "|{\"address\":\"5696 Young Station Suite 257\\nPort Philipfurt, MH 76613\",\"date\":\"2024-05-01T15:15:03.788Z\",\"dob\":\"1923-09-23\",\"email\":\"sean61@example.net\",\"name\":\"Matthew Richardson\",\"phone\":\"821-984-5954\",\"date_part\":\"20240501\"}    |\n",
            "|{\"address\":\"0231 Ward Prairie\\nWest Toddmouth, GU 81005\",\"date\":\"2024-05-04T03:44:59.935Z\",\"dob\":\"1972-03-31\",\"email\":\"wyattalyssa@example.com\",\"name\":\"Jessica Johnson\",\"phone\":\"(517)686-4146x548\",\"date_part\":\"20240504\"}         |\n",
            "|{\"address\":\"727 Julia Parks\\nNorth Louisberg, PW 39632\",\"date\":\"2024-05-04T11:12:08.318Z\",\"dob\":\"1958-08-21\",\"email\":\"ibarajas@example.com\",\"name\":\"Scott Allen\",\"phone\":\"899-731-1448\",\"date_part\":\"20240504\"}                      |\n",
            "|{\"address\":\"343 Vazquez Heights\\nNorth Howardmouth, DE 12888\",\"date\":\"2024-05-04T19:40:35.340Z\",\"dob\":\"1933-06-18\",\"email\":\"cody34@example.net\",\"name\":\"Wayne Goodman\",\"phone\":\"485-385-3350\",\"date_part\":\"20240504\"}                |\n",
            "|{\"address\":\"230 Laura Ridge Suite 258\\nSouth Kristin, ND 17931\",\"date\":\"2024-05-01T16:56:37.889Z\",\"dob\":\"1983-07-11\",\"email\":\"fgreen@example.com\",\"name\":\"Jamie Robinson\",\"phone\":\"+1-297-578-4389x09121\",\"date_part\":\"20240501\"}    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading json as text\n",
        "spark.read.json(path).show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bHcT2ilUo_F",
        "outputId": "fa7f17c4-cab9-45a1-9929-530222195a0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------+------------------------+---------+----------+-------------------------+------------------+---------------------+\n",
            "|address                                                |date                    |date_part|dob       |email                    |name              |phone                |\n",
            "+-------------------------------------------------------+------------------------+---------+----------+-------------------------+------------------+---------------------+\n",
            "|4973 Amber Ranch Apt. 834\\nStephensonmouth, CO 07625   |2024-05-03T11:37:52.167Z|20240503 |1918-06-08|odelacruz@example.org    |Dawn Ortega       |253.508.8977         |\n",
            "|883 King Villages\\nHamptonburgh, NJ 38378              |2024-05-03T09:31:21.718Z|20240503 |1993-02-26|yvega@example.com        |Arthur Mack       |2395131993           |\n",
            "|700 Nicole Path\\nSheilamouth, LA 62980                 |2024-05-03T20:57:51.270Z|20240503 |2014-11-29|torresrebecca@example.net|Justin Gonzalez   |9529406655           |\n",
            "|02437 Luna Highway Apt. 257\\nCervantesport, KS 08567   |2024-05-03T15:19:05.740Z|20240503 |2016-04-11|shelly25@example.org     |Catherine Scott   |(592)680-4984        |\n",
            "|41718 Donald Island Apt. 212\\nHernandezberg, AK 52302  |2024-05-02T08:21:44.077Z|20240502 |1929-08-07|candice41@example.net    |Jeremy Holmes     |001-950-609-8212x212 |\n",
            "|5696 Young Station Suite 257\\nPort Philipfurt, MH 76613|2024-05-01T15:15:03.788Z|20240501 |1923-09-23|sean61@example.net       |Matthew Richardson|821-984-5954         |\n",
            "|0231 Ward Prairie\\nWest Toddmouth, GU 81005            |2024-05-04T03:44:59.935Z|20240504 |1972-03-31|wyattalyssa@example.com  |Jessica Johnson   |(517)686-4146x548    |\n",
            "|727 Julia Parks\\nNorth Louisberg, PW 39632             |2024-05-04T11:12:08.318Z|20240504 |1958-08-21|ibarajas@example.com     |Scott Allen       |899-731-1448         |\n",
            "|343 Vazquez Heights\\nNorth Howardmouth, DE 12888       |2024-05-04T19:40:35.340Z|20240504 |1933-06-18|cody34@example.net       |Wayne Goodman     |485-385-3350         |\n",
            "|230 Laura Ridge Suite 258\\nSouth Kristin, ND 17931     |2024-05-01T16:56:37.889Z|20240501 |1983-07-11|fgreen@example.com       |Jamie Robinson    |+1-297-578-4389x09121|\n",
            "+-------------------------------------------------------+------------------------+---------+----------+-------------------------+------------------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# partition json data + saveAsTable\n",
        "\n",
        "# Creating partition column\n",
        "df = df.withColumn(\"date_part\", date_format(col(\"date\"), \"yyyyMMdd\"))\n",
        "\n",
        "# write as json\n",
        "(df.write\n",
        "  .partitionBy(\"date_part\")\n",
        "  .mode(\"overwrite\")\n",
        "  .format(\"json\")\n",
        "  .saveAsTable(\"tbl_json_part\"))\n",
        "\n",
        "# read as json\n",
        "print(spark.table(\"tbl_json_part\").count())\n",
        "\n",
        "# read as json\n",
        "spark.sql(\"show partitions tbl_json_part\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj59UNMuU0hV",
        "outputId": "186a9dfc-f786-40a3-c480-40764493074b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|date_part=20240501|\n",
            "|date_part=20240502|\n",
            "|date_part=20240503|\n",
            "|date_part=20240504|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Append Mode"
      ],
      "metadata": {
        "id": "6RhijzyqZeeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with APPEND\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_append\"\n",
        "\n",
        "df.write.mode(\"append\").format(\"parquet\").save(path)\n",
        "\n",
        "!ls /content/write_partitioning/parquet_append\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "id": "GmLjA1zDZeG_",
        "outputId": "669a935c-bd31-418d-a08a-d84937b6c8f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-018b8989-6340-420c-9f1f-510eb18f63f6-c000.snappy.parquet\n",
            "part-00000-02799fa2-e41b-4cdb-8ba6-3e6fb5fa1124-c000.snappy.parquet\n",
            "part-00000-18c62a08-1d06-4fb0-948d-5147b17e84dd-c000.snappy.parquet\n",
            "part-00000-1b715300-051a-4713-bdd7-45870cf73ea9-c000.snappy.parquet\n",
            "part-00000-1dd1f85b-f385-4f43-b747-169e6984bd39-c000.snappy.parquet\n",
            "part-00000-29fef1fd-0454-4699-91b2-82c720fac0e5-c000.snappy.parquet\n",
            "part-00000-46410ea7-14c1-47a8-ad15-02887f0f92be-c000.snappy.parquet\n",
            "part-00000-a57562f5-80c0-4e75-9fa2-35f8e396635e-c000.snappy.parquet\n",
            "part-00000-f3ea985a-863e-4db5-95da-8d4ded8b7104-c000.snappy.parquet\n",
            "part-00000-fa4b706b-9d71-4f99-ae7a-4c1d4f5283cb-c000.snappy.parquet\n",
            "_SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}